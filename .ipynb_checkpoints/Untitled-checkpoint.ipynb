{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode : 20, Avg timestep : 9.4, buffer size : 210, epsilon : 7.9%\n",
      "# of episode : 40, Avg timestep : 9.2, buffer size : 413, epsilon : 7.8%\n",
      "# of episode : 60, Avg timestep : 9.2, buffer size : 618, epsilon : 7.7%\n",
      "# of episode : 80, Avg timestep : 8.7, buffer size : 812, epsilon : 7.6%\n",
      "# of episode : 100, Avg timestep : 8.8, buffer size : 1009, epsilon : 7.5%\n",
      "# of episode : 120, Avg timestep : 8.4, buffer size : 1197, epsilon : 7.4%\n",
      "# of episode : 140, Avg timestep : 8.7, buffer size : 1390, epsilon : 7.3%\n",
      "# of episode : 160, Avg timestep : 8.2, buffer size : 1575, epsilon : 7.2%\n",
      "# of episode : 180, Avg timestep : 8.8, buffer size : 1772, epsilon : 7.1%\n",
      "# of episode : 200, Avg timestep : 8.6, buffer size : 1964, epsilon : 7.0%\n",
      "# of episode : 220, Avg timestep : 9.1, buffer size : 2165, epsilon : 6.9%\n",
      "# of episode : 240, Avg timestep : 13.4, buffer size : 2453, epsilon : 6.8%\n",
      "# of episode : 260, Avg timestep : 8.9, buffer size : 2652, epsilon : 6.7%\n",
      "# of episode : 280, Avg timestep : 11.7, buffer size : 2905, epsilon : 6.6%\n",
      "# of episode : 300, Avg timestep : 19.9, buffer size : 3322, epsilon : 6.5%\n",
      "# of episode : 320, Avg timestep : 70.5, buffer size : 4751, epsilon : 6.4%\n",
      "# of episode : 340, Avg timestep : 97.8, buffer size : 6726, epsilon : 6.3%\n",
      "# of episode : 360, Avg timestep : 91.7, buffer size : 8579, epsilon : 6.2%\n",
      "# of episode : 380, Avg timestep : 131.2, buffer size : 11223, epsilon : 6.1%\n",
      "# of episode : 400, Avg timestep : 162.2, buffer size : 14487, epsilon : 6.0%\n",
      "# of episode : 420, Avg timestep : 147.1, buffer size : 17449, epsilon : 5.9%\n",
      "# of episode : 440, Avg timestep : 133.5, buffer size : 20139, epsilon : 5.8%\n",
      "# of episode : 460, Avg timestep : 218.9, buffer size : 24537, epsilon : 5.7%\n",
      "# of episode : 480, Avg timestep : 197.9, buffer size : 28516, epsilon : 5.6%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-89f67a51c200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-89f67a51c200>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# game이 끝난 step이면 0이고 아니면 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reinforce/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/reinforce/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mthetaacc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta_dot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque()\n",
    "        self.batch_size = 32            # replay buffer에서 sample을 해야\n",
    "        self.size_limit = 500000        # buffer의 최대 크기\n",
    "\n",
    "    def put(self, data):\n",
    "        self.buffer.append(data)\n",
    "        # 오른쪽에 넣으니깐 왼쪽에서 빼라\n",
    "        if len(self.buffer) > self.size_limit:\n",
    "            self.buffer.popleft()\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self.buffer, n)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 마지막 단에서는 relu는 안넣음\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return out.argmax().item()\n",
    "\n",
    "def train(q, q_target, memory, gamma, optimizer, batch_size):\n",
    "    # 한 episode 끝날때 마다 train 함수가 호출이 되는데 한번만 업데이트하는 것보다 10번 뽑아서 업데이트해라\n",
    "    for i in range(10):\n",
    "        batch = memory.sample(batch_size)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "\n",
    "        for transition in batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])   #dimension 맞추기위해(shape)\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        s, a, r, s_prime, done_mask = torch.tensor(s_lst, dtype = torch.float), \\\n",
    "                                      torch.tensor(a_lst), \\\n",
    "                                      torch.tensor(r_lst), \\\n",
    "                                      torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                      torch.tensor(done_mask_lst)\n",
    "\n",
    "        # 32개의 state shape:[32,4] batchsize가 32이고, s는 원래 4였음. => q(s)의 shape [32,2]\n",
    "        # batch 처리를 하면 굉장히 빠르다.\n",
    "        q_out = q(s)\n",
    "\n",
    "        # q_out에는 각 양쪽의 q value가 있는데 그 중 하나의 action을 선택하기 위해서\n",
    "        # q_a의 shape : [32,1]\n",
    "        # gather(1,a)의 1은 a에서 0번째 축말고 1번째 축의 데이터를 뽑기 위해\n",
    "        q_a = q_out.gather(1,a)\n",
    "\n",
    "        # 다음 state를 q target의 input으로 넣어 나온 value들이 있다.\n",
    "        # q_target shape : [32, 2]\n",
    "        # max하면 [32,]\n",
    "        # unsqueeze하면 [32, 1]\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # 마지막 step에서는 다음에 reward를 받으면 안된다 그래서 done_mask를 이용\n",
    "        # TD target은 s에서 a를 한 s'에서의 q를 이용하는건데 q learning은 s'에서의 max값을 구함으로 max_q_prime을 사용\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "\n",
    "        # target과 q_a의 차이가 loss\n",
    "        loss = F.smooth_l1_loss(target, q_a)\n",
    "\n",
    "        # optimizer의 gradient를 비워준다.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "    # q와 q_target으로 target network를 구현\n",
    "    q = Qnet()\n",
    "    q_target = Qnet()\n",
    "\n",
    "    # q를 target q로 복사\n",
    "    # state_dict는 model의 weight 정보를 dictionay 형태로 닮고 있음\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    avg_t = 0\n",
    "    gamma = 0.98\n",
    "    batch_size = 32\n",
    "    render = False\n",
    "\n",
    "    # q만 update한다. q target은 고정되어 있다가 q의 정보를 가지고온다.\n",
    "    optimizer = optim.Adam(q.parameters(), lr = 0.0005)\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n",
    "        s = env.reset()\n",
    "\n",
    "        for t in range(600):\n",
    "            a = q.sample_action(torch.from_numpy(s).float(), epsilon)\n",
    "            s_prime, r, done, info = env.step(a)\n",
    "\n",
    "            # game이 끝난 step이면 0이고 아니면 1\n",
    "            # TD target 곱할때 테크\n",
    "            done_mark = 0.0 if done else 1.0\n",
    "            memory.put((s, a, r/200, s_prime, done_mark))\n",
    "            s = s_prime\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        avg_t += t\n",
    "        \n",
    "        if avg_t/20.0 > 300:\n",
    "            render = True\n",
    "\n",
    "        # 메모리에 어느정도 축적이 된 상태에서 train 해야한다.\n",
    "        if memory.size() > 2000:\n",
    "            train(q, q_target, memory, gamma, optimizer, batch_size)\n",
    "\n",
    "        # 20번 episode마다 q target network를 업데이트한다.\n",
    "        if n_epi%20==0 and n_epi!=0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "            print(\"# of episode : {}, Avg timestep : {:.1f}, buffer size : {}, epsilon : {:.1f}%\".format(\n",
    "                n_epi, avg_t/20.0, memory.size(), epsilon*100\n",
    "            ))\n",
    "\n",
    "            avg_t = 0\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce",
   "language": "python",
   "name": "reinforce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
