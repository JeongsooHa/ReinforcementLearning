{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "def zeros(shape, dtype=tf.float32, scope='default'):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        init = tf.zeros(shape, dtype=dtype)\n",
    "        return tf.Variable(init)\n",
    "    \n",
    "def glorot(shape, dtype=tf.float32, scope='default'):\n",
    "    # Xavier Glorot & Yoshua Bengio (AISTATS 2010) initialization (Eqn 16)\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        init_range = np.sqrt(6.0 / (shape[0] + shape[1]))\n",
    "        init = tf.random.uniform(\n",
    "            shape, minval=-init_range, maxval=init_range, dtype=dtype)\n",
    "        return tf.Variable(init)\n",
    "\n",
    "def leaky_relu(features, alpha=0.2, name=None):\n",
    "    \"\"\"Compute the Leaky ReLU activation function.\n",
    "    \"Rectifier Nonlinearities Improve Neural Network Acoustic Models\"\n",
    "    AL Maas, AY Hannun, AY Ng - Proc. ICML, 2013\n",
    "    http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf\n",
    "    Args:\n",
    "    features: A `Tensor` representing preactivation values.\n",
    "    alpha: Slope of the activation function at x < 0.\n",
    "    name: A name for the operation (optional).\n",
    "    Returns:\n",
    "    The activation value.\n",
    "    \"\"\"\n",
    "    with ops.name_scope(name, \"LeakyRelu\", [features, alpha]):\n",
    "        features = ops.convert_to_tensor(features, name=\"features\")\n",
    "        alpha = ops.convert_to_tensor(alpha, name=\"alpha\")\n",
    "        return math_ops.maximum(alpha * features, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphCNN_PE(object):\n",
    "    def __init__(self, inputs, input_dim, hid_dims, output_dim, max_depth, act_fn, scope='gcn'):\n",
    "\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dims = hid_dims\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.max_depth = max_depth  # hyperparam\n",
    "\n",
    "        self.act_fn = act_fn\n",
    "        self.scope = scope\n",
    "\n",
    "        # message passing\n",
    "        self.adj_mats = [tf.compat.v1.sparse_placeholder(\n",
    "            tf.float32, [None, None]) for _ in range(self.max_depth)]\n",
    "        self.masks = [tf.compat.v1.placeholder(\n",
    "            tf.float32, [None, 1]) for _ in range(self.max_depth)]\n",
    "\n",
    "        # initialize message passing transformation parameters\n",
    "        # h: x -> x'\n",
    "        self.prep_weights, self.prep_bias = \\\n",
    "            self.init(self.input_dim, self.hid_dims, self.output_dim)\n",
    "\n",
    "        # f: x' -> e\n",
    "        self.proc_weights, self.proc_bias = \\\n",
    "            self.init(self.output_dim, self.hid_dims, self.output_dim)\n",
    "\n",
    "        # g: e -> e\n",
    "        self.agg_weights, self.agg_bias = \\\n",
    "            self.init(self.output_dim, self.hid_dims, self.output_dim)\n",
    "\n",
    "        # graph message passing\n",
    "        self.outputs = self.forward()\n",
    "\n",
    "    def init(self, input_dim, hid_dims, output_dim):\n",
    "        # Initialize the parameters\n",
    "        # these weights may need to be re-used\n",
    "        # e.g., we may want to propagate information multiple times\n",
    "        # but using the same way of processing the nodes\n",
    "        weights = []\n",
    "        bias = []\n",
    "\n",
    "        curr_in_dim = input_dim\n",
    "\n",
    "        # hidden layers\n",
    "        for hid_dim in hid_dims:\n",
    "            weights.append(\n",
    "                glorot([curr_in_dim, hid_dim], scope=self.scope))\n",
    "            bias.append(\n",
    "                zeros([hid_dim], scope=self.scope))\n",
    "            curr_in_dim = hid_dim\n",
    "\n",
    "        # output layer\n",
    "        weights.append(glorot([curr_in_dim, output_dim], scope=self.scope))\n",
    "        bias.append(zeros([output_dim], scope=self.scope))\n",
    "\n",
    "        return weights, bias\n",
    "\n",
    "    def forward(self):\n",
    "        # message passing among nodes\n",
    "        # the information is flowing from leaves to roots\n",
    "        x = self.inputs\n",
    "\n",
    "        # raise x into higher dimension\n",
    "        for l in range(len(self.prep_weights)):\n",
    "            x = tf.matmul(x, self.prep_weights[l])\n",
    "            x += self.prep_bias[l]\n",
    "            x = self.act_fn(x)\n",
    "\n",
    "        for d in range(self.max_depth):\n",
    "            # work flow: index_select -> f -> masked assemble via adj_mat -> g\n",
    "            y = x\n",
    "\n",
    "            # process the features on the nodes\n",
    "            for l in range(len(self.proc_weights)):\n",
    "                y = tf.matmul(y, self.proc_weights[l])\n",
    "                y += self.proc_bias[l]\n",
    "                y = self.act_fn(y)\n",
    "\n",
    "            # message passing\n",
    "            y = tf.sparse.sparse_dense_matmul(self.adj_mats[d], y)\n",
    "\n",
    "            # aggregate child features\n",
    "            for l in range(len(self.agg_weights)):\n",
    "                y = tf.matmul(y, self.agg_weights[l])\n",
    "                y += self.agg_bias[l]\n",
    "                y = self.act_fn(y)\n",
    "\n",
    "            # remove the artifact from the bias term in g\n",
    "            y = y * self.masks[d]\n",
    "\n",
    "            # assemble neighboring information\n",
    "            x = x + y\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe predict\n",
    "pe_inputs = np.zeros((5, 4), dtype=np.float32)\n",
    "pe_input_dim = 4\n",
    "hid_dims = [16,8]\n",
    "output_dim = 8\n",
    "max_depth = 2\n",
    "act_fn = leaky_relu\n",
    "scope='gcn_pe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_pe = GraphCNN_PE(pe_inputs, pe_input_dim, hid_dims, output_dim, max_depth, act_fn, scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GraphCNN_PE at 0x1c458c1b50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "gcn_pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pe gradient\n",
    "pe_inputs = np.zeros((16, 4), dtype=np.float32)\n",
    "pe_input_dim = 4\n",
    "hid_dims = [16,8]\n",
    "output_dim = 8\n",
    "max_depth = 2\n",
    "act_fn = leaky_relu\n",
    "scope='gcn_pe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_pe = GraphCNN_PE(pe_inputs, pe_input_dim, hid_dims, output_dim, max_depth, act_fn, scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch12",
   "language": "python",
   "name": "torch12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
