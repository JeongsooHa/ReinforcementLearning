diff --git a/a3c/__pycache__/envs.cpython-37.pyc b/a3c/__pycache__/envs.cpython-37.pyc
new file mode 100644
index 0000000..12f3b5c
Binary files /dev/null and b/a3c/__pycache__/envs.cpython-37.pyc differ
diff --git a/a3c/__pycache__/model.cpython-37.pyc b/a3c/__pycache__/model.cpython-37.pyc
new file mode 100644
index 0000000..c0c2aa0
Binary files /dev/null and b/a3c/__pycache__/model.cpython-37.pyc differ
diff --git a/a3c/__pycache__/my_optim.cpython-37.pyc b/a3c/__pycache__/my_optim.cpython-37.pyc
new file mode 100644
index 0000000..e1a9290
Binary files /dev/null and b/a3c/__pycache__/my_optim.cpython-37.pyc differ
diff --git a/a3c/__pycache__/test.cpython-37.pyc b/a3c/__pycache__/test.cpython-37.pyc
new file mode 100644
index 0000000..86cd685
Binary files /dev/null and b/a3c/__pycache__/test.cpython-37.pyc differ
diff --git a/a3c/__pycache__/train.cpython-37.pyc b/a3c/__pycache__/train.cpython-37.pyc
new file mode 100644
index 0000000..add1e87
Binary files /dev/null and b/a3c/__pycache__/train.cpython-37.pyc differ
diff --git a/a3c/main.py b/a3c/main.py
index 235e65f..06f2ece 100644
--- a/a3c/main.py
+++ b/a3c/main.py
@@ -9,7 +9,6 @@ from model import ActorCritic
 from test import test
 from train import train
 
-torch.device('cpu')
 
 parser = argparse.ArgumentParser(description='A3C')
 parser.add_argument('--lr', type=float, default=0.0001,
@@ -26,9 +25,9 @@ parser.add_argument('--max-grad-norm', type=float, default=50,
                     help='value loss coefficient (default: 50)')
 parser.add_argument('--seed', type=int, default=1,
                     help='random seed (default: 1)')
-parser.add_argument('--num-processes', type=int, default=0,
+parser.add_argument('--num-processes', type=int, default=4,
                     help='how many training processes to use (default: 4)')
-parser.add_argument('--num-steps', type=int, default=20,
+parser.add_argument('--num-steps', type=int, default=100,
                     help='number of forward steps in A3C (default: 20)')
 parser.add_argument('--max-episode-length', type=int, default=1000000,
                     help='maximum length of an episode (default: 1000000)')
@@ -36,6 +35,8 @@ parser.add_argument('--env-name', default='PongDeterministic-v4',
                     help='environment to train on (default: PongDeterministic-v4)')
 parser.add_argument('--no-shared', default=False,
                     help='use an optimizer without shared momentum.')
+parser.add_argument('--wandb', default=True,
+                    help='wandb option.')
 
 if __name__ == '__main__':
     os.environ['OMP_NUM_THREADS'] = '1'
@@ -51,6 +52,9 @@ if __name__ == '__main__':
         env.observation_space.shape[0], env.action_space
     )
     shared_model.share_memory()
+    if args.wandb:
+        import wandb
+        wandb.init(project="reinforce")
 
     if args.no_shared:
         optimizer = None
diff --git a/a3c/test.py b/a3c/test.py
index b030716..c117f64 100644
--- a/a3c/test.py
+++ b/a3c/test.py
@@ -1,5 +1,6 @@
 import time
 from collections import deque
+import wandb
 
 import torch
 import torch.nn.functional as F
@@ -53,6 +54,9 @@ def test(rank, args, shared_model, counter):
         if actions.count(actions[0]) == actions.maxlen:
             done = True
 
+        if args.wandb:
+            wandb.log({"test reward": reward_sum})
+
         if done:
             print("Time {}, num steps {}, FPS {:.0f}, episode reward {}, episode length {}".format(
                 time.strftime("%Hh %Mm %Ss",
diff --git a/a3c/train.py b/a3c/train.py
index 27b9c0f..2ba9a66 100644
--- a/a3c/train.py
+++ b/a3c/train.py
@@ -1,6 +1,7 @@
 import torch
 import torch.nn.functional as F
 import torch.optim as optim
+import wandb
 
 from envs import create_atari_env
 from model import ActorCritic
@@ -108,4 +109,10 @@ def train(rank, args, shared_model, counter, lock, optimizer=None):
         torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)
 
         ensure_shared_grads(model, shared_model)
-        optimizer.step()
\ No newline at end of file
+        optimizer.step()
+
+        if args.wandb:
+            wandb.log({"policy loss": policy_loss,
+                       "gae":gae,
+                       "reward":sum(rewards)})
+
